# DIGITAL ARCHAEOLOGY - COMPLETE SYSTEM EXPLANATION

## üèóÔ∏è SYSTEM ARCHITECTURE OVERVIEW

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         USER BROWSER                         ‚îÇ
‚îÇ        (Frontend: HTML, CSS, JavaScript at port 3000)        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚Ä¢ Upload documents                                           ‚îÇ
‚îÇ  ‚Ä¢ Search/Query                                               ‚îÇ
‚îÇ  ‚Ä¢ Browse (Explore) all documents                            ‚îÇ
‚îÇ  ‚Ä¢ Track indexing jobs                                        ‚îÇ
‚îÇ  ‚Ä¢ View notifications                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚îÇ HTTP/API Calls
          ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ             FastAPI Backend (Port 8001)                      ‚îÇ
‚îÇ                    (app.py)                                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Endpoints:                                                    ‚îÇ
‚îÇ ‚Ä¢ POST /upload        ‚Üí Upload & index files                ‚îÇ
‚îÇ ‚Ä¢ GET /search         ‚Üí Semantic search                     ‚îÇ
‚îÇ ‚Ä¢ GET /explore        ‚Üí Browse all documents                ‚îÇ
‚îÇ ‚Ä¢ GET /download       ‚Üí Download original files             ‚îÇ
‚îÇ ‚Ä¢ GET /notifications  ‚Üí Get activity log                    ‚îÇ
‚îÇ ‚Ä¢ GET /status         ‚Üí System stats                        ‚îÇ
‚îÇ ‚Ä¢ GET /job/{id}       ‚Üí Track background jobs              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚îÇ Uses modules
          ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ            ‚îÇ          ‚îÇ            ‚îÇ
    ‚ñº            ‚ñº          ‚ñº            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ OCR.py ‚îÇ  ‚îÇEMBED.py‚îÇ ‚îÇ  DB.py   ‚îÇ ‚îÇSEARCH.py ‚îÇ
‚îÇ        ‚îÇ  ‚îÇ        ‚îÇ ‚îÇ          ‚îÇ ‚îÇ          ‚îÇ
‚îÇ‚Ä¢ PDF   ‚îÇ  ‚îÇ‚Ä¢ Vector‚îÇ ‚îÇ‚Ä¢SQLite   ‚îÇ ‚îÇ(embedded)‚îÇ
‚îÇ‚Ä¢ Image ‚îÇ  ‚îÇ  Embed ‚îÇ ‚îÇ‚Ä¢Persist  ‚îÇ ‚îÇ          ‚îÇ
‚îÇExtract ‚îÇ  ‚îÇ‚Ä¢ FAISS ‚îÇ ‚îÇ  Data    ‚îÇ ‚îÇ          ‚îÇ
‚îÇText    ‚îÇ  ‚îÇ Search ‚îÇ ‚îÇ          ‚îÇ ‚îÇ          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìö MAIN COMPONENTS EXPLAINED

### 1Ô∏è‚É£ **FRONTEND (HTML/CSS/JavaScript at port 3000)**

**Purpose**: User interface for interacting with the system

**Key Sections**:

#### a) **Upload Section**
```javascript
uploadFile() - Sends PDF/image to backend
- Reads file from user's computer
- Sends to POST /upload endpoint
- Tracks job progress with UUID
- Shows status: "Indexing...", "Complete!", or "Error"
```

#### b) **Search Section**
```javascript
searchText() - Queries indexed documents
- User enters search term (min 3 characters)
- Calls GET /search?query=...
- Returns matching documents with snippets
- Shows "not found" for non-existent items (after strict filtering)
```

#### c) **Explore Section** (NEW)
```javascript
loadExplore() - Browse all indexed documents
- Pagination: 10 documents per page
- Navigate with Next/Previous buttons
- Shows document count and progress
- Can view full details of each document
```

#### d) **Notifications Section**
```javascript
fetchNotifications() - Real-time activity log
- Polls backend every 8 seconds
- Shows recent uploads, searches, downloads
- Displays timestamps and status messages
```

---

### 2Ô∏è‚É£ **BACKEND - app.py (FastAPI Server)**

**Purpose**: HTTP API for all operations, orchestrates requests

**Key Endpoints**:

```python
# UPLOAD: Receives files and schedules indexing
@app.post("/upload")
- Saves file to disk
- Extracts text with OCR
- Returns async job ID (non-blocking)
- Indexing happens in background thread

# SEARCH: Semantic search with strict quality filtering
@app.get("/search?query=...")
- Calls embed.search_text(query)
- Returns top 3 most relevant documents
- Filters out low-quality matches (strict thresholds)

# EXPLORE: Browse paginated documents
@app.get("/explore?limit=10&offset=0")
- Returns paginated document list
- Includes pagination info (total, has_more)
- Shows document metadata and snippets

# DOWNLOAD: Serves original uploaded files
@app.get("/download?filename=...")
- Prevents path traversal attacks
- Returns original PDF/image file

# STATUS: System information
@app.get("/status")
- Returns: document count, vector count
- Shows how many documents are indexed

# JOB TRACKING: Monitor background uploads
@app.get("/job/{job_id}")
- Status: pending, running, done, failed
- doc_id, error message, timestamps

# NOTIFICATIONS: Activity log
@app.get("/notifications")
- Returns last 100 notification messages
- Merged from DB + in-memory log
```

---

### 3Ô∏è‚É£ **EMBED.py - Vector Search Engine**

**Purpose**: Semantic search using AI embeddings and vector database

#### **Key Concepts**:

**A) SENTENCE TRANSFORMER MODEL** (all-MiniLM-L6-v2)
```
What is it?
- AI model that converts text to vectors (384-dimensional)
- Understands meaning/semantics of text
- Similar texts get similar vectors

Example:
  "University notice"    ‚Üí [0.12, 0.45, -0.23, ..., 0.87]
  "Academic announcement" ‚Üí [0.14, 0.48, -0.21, ..., 0.89]
  (These vectors are SIMILAR because meaning is similar)

  "Pizza recipe"        ‚Üí [0.92, -0.15, 0.67, ..., -0.34]
  (This vector is DIFFERENT - completely different topic)
```

**B) FAISS INDEX** (Vector Database)
```
What is it?
- Fast Approximate Nearest Neighbor Search
- Stores vectors in optimized format for quick lookup
- IndexFlatL2 = finds closest 50 vectors to query vector

Process:
1. Convert query to vector
2. Search FAISS for closest 50 vectors
3. Return documents those vectors came from
4. Re-rank by quality
```

**C) DOCUMENT CHUNKING**
```
Why chunk?
- Large documents split into 500-char chunks
- Each chunk gets its own vector
- Better granularity for search

Example:
  Original: "This is a university notice about admission. 
             The deadline is March 15th. Apply now."
  
  Chunk 1: "This is a university notice about admission."
  Chunk 2: "The deadline is March 15th. Apply now."
  
  Each chunk embedded separately
```

#### **Multi-Stage Search Pipeline**:

```
Stage 1: FAST EXACT MATCHING
‚îú‚îÄ Scan first 5000 documents
‚îú‚îÄ Look for exact phrase/token matches
‚îî‚îÄ Return early if found (skip expensive FAISS)

Stage 2: SEMANTIC SEARCH (if no exact matches)
‚îú‚îÄ Convert query to vector
‚îú‚îÄ Find 50 closest vectors in FAISS
‚îú‚îÄ Retrieve documents for those vectors
‚îî‚îÄ Proceed to re-ranking

Stage 3: RE-RANKING & SCORING (Comprehensive Relevance Check)
‚îú‚îÄ Signal 1: Semantic Similarity (75% weight)
‚îÇ   ‚îî‚îÄ How well query vector matches document vector
‚îú‚îÄ Signal 2: Exact Term Matching (20% weight)
‚îÇ   ‚îî‚îÄ How many query words appear in document
‚îî‚îÄ Signal 3: Title Bonus (5% weight)
    ‚îî‚îÄ Boost if query words in title

Score = (0.75 √ó semantic) + (0.20 √ó exact) + (0.05 √ó title)

Stage 4: STRICT QUALITY FILTERING (NEW)
‚îú‚îÄ Semantic similarity must be > 0.45 (threshold)
‚îú‚îÄ Relevance score must be > 0.35 (threshold)
‚îú‚îÄ OR: Document must have exact query terms
‚îî‚îÄ Result: Invalid = skip, Valid = include

Stage 5: DEDUPLICATION & FORMATTING
‚îú‚îÄ Remove duplicate sources
‚îú‚îÄ Limit to top 3 results
‚îú‚îÄ Format response with snippets
‚îî‚îÄ Return to user
```

#### **Scoring Weights** (How importance is distributed):
```python
WEIGHT_SEMANTIC = 0.35  # Primary signal
WEIGHT_EXACT = 0.30     # Token presence
WEIGHT_DENSITY = 0.08   # Term frequency
WEIGHT_TITLE = 0.12     # Title importance
WEIGHT_SOURCE = 0.10    # Source relevance
WEIGHT_CONTEXT = 0.05   # Surrounding text
```

#### **Relevance Score Calculation**:
```python
def _calculate_relevance_score(query_words, doc_text, semantic_sim, title):
    # Signal 1: Semantic (AI model evaluated similarity)
    semantic_score = min(semantic_sim, 1.0)
    
    # Signal 2: How many query words in document?
    matches = sum(1 for w in query_words if w in doc_text.lower())
    exact_score = min(matches / len(query_words), 1.0)
    
    # Signal 3: Did query words appear in title?
    title_score = 0.3 if any(w in title.lower() for w in query_words) else 0.0
    
    # Weighted combination
    combined = (0.75 √ó semantic_score) + (0.20 √ó exact_score) + (0.05 √ó title_score)
    return combined  # 0.0 to 1.0 scale
```

#### **Strict Filtering Thresholds**:
```python
# For a result to be included:
min_semantic = semantic_sim > 0.45  # Must be moderately similar
minimum_relevance = relevance_score > 0.35

# BOTH conditions required:
if not (min_semantic or has_exact_terms):
    continue  # Skip this result
if relevance_score < 0.35:
    continue  # Skip low-quality result
```

---

### 4Ô∏è‚É£ **OCR.py - Text Extraction**

**Purpose**: Extract text from PDFs and images

**Process**:
```
1. User uploads PDF/PNG/JPG
2. EasyOCR reads image (primary method)
3. If fails, Tesseract tries (fallback)
4. Returns: (raw_text, cleaned_text)
   - raw_text: Exact extraction with all OCR noise
   - cleaned_text: Cleaned up for searching
```

**Text Cleaning**:
```python
# Remove code-like symbols
"Hello {test}" ‚Üí "Hello test"

# Fix spacing
"Hello   world" ‚Üí "Hello world"

# Remove path characters
"file/path\to\doc" ‚Üí "file path to doc"
```

---

### 5Ô∏è‚É£ **DB.py - Database (SQLite)**

**Purpose**: Persistent storage of documents and notifications

**Database Schema**:

```sql
-- Documents Table
CREATE TABLE documents (
    id INTEGER PRIMARY KEY,
    raw TEXT,              -- Original OCR output
    clean TEXT,            -- Cleaned for search
    source TEXT,           -- Filename
    url TEXT,              -- Original URL
    title TEXT,            -- Document title
    filename TEXT          -- Upload filename
);

-- Notifications Table
CREATE TABLE notifications (
    id INTEGER PRIMARY KEY,
    message TEXT,          -- Activity message
    time TIMESTAMP         -- When it happened
);
```

**Operations**:
- Save documents after OCR extraction
- Persist notifications for activity log
- Load documents on startup (cache in memory)
- Enable data recovery after restart

---

## üîÑ COMPLETE DATA FLOW

### **Upload Flow**:
```
1. User selects PDF/Image
   ‚Üì
2. Frontend sends to POST /upload
   ‚Üì
3. Backend saves file to disk
   ‚Üì
4. OCR.py extracts text (raw + cleaned)
   ‚Üì
5. Document saved to SQLite database
   ‚Üì
6. EMBED.py chunks text (500 chars each)
   ‚Üì
7. SentenceTransformer converts chunks to vectors
   ‚Üì
8. FAISS index stores vectors
   ‚Üì
9. Return job_id to frontend (non-blocking)
   ‚Üì
10. Backend continues indexing in background
    ‚Üì
11. Frontend polls /job/{id} for status
    ‚Üì
12. User sees "Complete!" notification
```

### **Search Flow**:
```
1. User enters query: "university admission deadline"
   ‚Üì
2. Frontend sends to GET /search?query=...
   ‚Üì
3. EMBED.py.retrieve() called:
   
   a) Stage 1: Check exact matches in first 5000 docs
      - Found? ‚Üí Return immediately, skip rest
      - Not found? ‚Üí Continue to Stage 2
   
   b) Stage 2: Semantic search
      - Convert query to 384-dim vector
      - Find 50 closest vectors in FAISS
      - Get documents for those 50
   
   c) Stage 3: Re-rank all candidates
      - Calculate relevance score for each
      - Sort by score descending
   
   d) Stage 4: Apply strict filters
      - Semantic similarity > 0.45? YES
      - Relevance score > 0.35? YES
      - Has exact query terms? YES
      - All pass? Include result
      - Failed? Skip result
   
   e) Stage 5: Prepare results
      - Remove duplicates
      - Limit to top 3
      - Format with snippets
   
4. Return 0-3 documents to frontend
   ‚Üì
5. If 0 results: Show "Item not found"
   ‚Üì
6. If >0 results: Show document cards with scores
```

### **Explore Flow**:
```
1. User clicks "Load Documents"
   ‚Üì
2. Frontend calls GET /explore?limit=10&offset=0
   ‚Üì
3. Backend returns:
   {
     "documents": [
       {doc_id, title, source, snippet, url},
       {doc_id, title, source, snippet, url},
       ... 10 total
     ],
     "total": 1336,
     "offset": 0,
     "limit": 10,
     "has_more": true
   }
   ‚Üì
4. Frontend displays first 10 documents with pagination
   ‚Üì
5. User clicks "Next" ‚Üí loads offset=10
   ‚Üì
6. Continue until offset + 10 >= total
```

---

## üìä IMPORTANT TECHNICAL TERMS DEFINED

| Term | Definition | Example |
|------|-----------|---------|
| **Vector** | Array of numbers representing text meaning | [0.12, -0.45, 0.23, ..., 0.87] with 384 dimensions |
| **Embedding** | Process of converting text to vector | "Hello world" ‚Üí [0.12, 0.45, ...] |
| **Semantic** | Meaning/context of text, not just keywords | "car" & "automobile" are semantically similar |
| **FAISS** | Fast vector database for similarity search | Finds 50 closest vectors in milliseconds |
| **Tokenization** | Splitting text into words/tokens | "Hello world" ‚Üí ["Hello", "world"] |
| **Chunk** | Small piece of text (500 chars) | Document split into overlapping pieces |
| **Relevance Score** | Numerical rating of how well result matches query | 0.0 (irrelevant) to 1.0 (perfect match) |
| **Threshold** | Minimum cutoff value | Semantic > 0.45: only include strong matches |
| **Levenshtein Distance** | Count of edits needed to change one word to another | "cat" ‚Üí "cut" = 1 edit (distance=1) |
| **Fuzzy Matching** | Finding similar-but-not-identical text | "recieve" matches "receive" |
| **OCR** | Optical Character Recognition - reading text from images | PDF ‚Üí extracted text |
| **Query** | User's search input | "university admission" |
| **Snippet** | Short preview of document text | First 200 characters |
| **Pagination** | Breaking results into pages | Show 10 at a time, navigate with Next/Previous |
| **Async** | Non-blocking operation, happens in background | Upload returns immediately, indexing continues |
| **Job Tracking** | Monitor status of background tasks | Check /job/{id} to see progress |

---

## ‚öôÔ∏è PERFORMANCE OPTIMIZATIONS IMPLEMENTED

### **1. Batch Encoding**
```python
# SLOW: Encode one by one
for chunk in chunks:
    embedding = model.encode(chunk)

# FAST: Batch all at once
embeddings = model.encode(chunks, batch_size=32)
# 32x+ faster due to GPU parallelization
```

### **2. Early Stop Search**
```python
# Don't search FAISS if exact match found
if exact_results:
    return exact_results  # Skip expensive FAISS
# Saves 90% time when exact matches exist
```

### **3. Limited Pre-filtering**
```python
# Don't scan all documents
MAX_PREFILTER_DOCS = min(len(documents), 5000)
# Scan only first 5000 instead of entire database
```

### **4. Simplied Scoring**
```python
# Use only 3 fast signals instead of 6 expensive ones
# Removed fuzzy matching in main loop
# Calculation time: ~1ms vs ~500ms
```

### **5. Pagination**
```python
# Don't return all 1336 documents
# Return 10 per page, total time ~37ms
```

### **6. Async Uploads**
```python
# Don't wait for indexing to complete
job_id = schedule_background_job(...)
return {"job_id": job_id}  # Return immediately
# User doesn't wait for 30-minute indexing
```

---

## üìà SYSTEM STATISTICS

```
Documents Indexed:     1,336
Vector Dimension:      384 (from all-MiniLM-L6-v2)
FAISS Candidates:      50 per search
Search Time:           ~37ms average
Strict Filtering:      Semantic > 0.45, Score > 0.35
Relevance Method:      3-signal weighted combination
Max Results Returned:  3 per search
Pagination:            10 documents per page
```

---

## üéØ WHY THIS ARCHITECTURE?

### **Why Semantic Search?**
- Keyword matching fails for synonyms
  - "car" query won't find "vehicle" documents
  - Embedding fixes this: similar meaning = similar vectors

### **Why FAISS?**
- SQLite text search too slow for 1336+ documents
- FAISS finds closest 50 vectors in milliseconds
- Standard solution for semantic search at scale

### **Why Strict Filtering?**
- Initial design returned 9 results (irrelev ones)
- Thresholds too low: 0.25 semantic, 0.15 score
- Increased to 0.45 and 0.35
- Now returns 0-3 results, all highly relevant
- Users see "not found" instead of random documents

### **Why Chunking?**
- Large documents won't match queries on edges
- Break into 500-char overlapping pieces
- Better granularity for semantic matching
- Each chunk independently searchable

### **Why 3-Signal Scoring?**
- Semantic alone misses keyword importance
- Exact matching alone misses nuance
- Combined score captures both
- Fast calculation (1ms per document)

### **Why Background Indexing?**
- PDF extraction: 10-30 seconds
- Large files need time
- Users don't want to wait
- Return immediately, update in background
- Show progress via job tracking

---

## üöÄ HOW TO USE THE SYSTEM

### **Upload a Document**
```
1. Click "Upload Document"
2. Select PDF or image
3. Click "Upload & Index"
4. Get job_id immediately
5. Frontend polls /job/{id}
6. See "Complete!" when done
7. Document now searchable
```

### **Search Documents**
```
1. Type 3+ characters minimum
2. Click "Search"
3. Get 0-3 highly relevant results
4. View snippets and source
5. If empty: document doesn't exist
```

### **Explore All Documents**
```
1. Click "Load Documents"
2. See page 1 (docs 1-10)
3. Click "Next" to see more
4. Click "Previous" to go back
5. Browse all 1336 documents
```

### **Download Original File**
```
1. Find document in search/explore
2. Click "Download" button
3. Original PDF/image saves to computer
```

---

## üêõ COMMON ISSUES & SOLUTIONS

| Issue | Cause | Solution |
|-------|-------|----------|
| Search returns empty | Thresholds too strict (>0.45) | Query words must appear in documents |
| Search very slow | FAISS not cached | Restart server to load FAISS |
| Upload seems stuck | Background job running | Check /job/{id} status endpoint |
| OCR text looks wrong | Poor image quality | Ensure PDF/image is clear |
| Can't find document | Exact words not in text | Try broader search terms |

---

## üìù CODE QUALITY FEATURES

‚úÖ **Type Hints**: Function signatures show parameter types
‚úÖ **Docstrings**: Explain what each function does
‚úÖ **Error Handling**: Try-catch blocks prevent crashes
‚úÖ **Logging**: Console messages show what's happening
‚úÖ **Comments**: Inline explanations for complex logic
‚úÖ **Modular Design**: Separate files for concerns (OCR, Search, DB)
‚úÖ **CORS Support**: Frontend can call backend from different port
‚úÖ **Thread Safety**: Locks prevent race conditions in background jobs

---

**System Built**: February 2026  
**Language**: Python 3.13  
**Framework**: FastAPI  
**AI Model**: Sentence Transformers (all-MiniLM-L6-v2)  
**Vector DB**: FAISS  
**SQLite Version**: 3.x  
